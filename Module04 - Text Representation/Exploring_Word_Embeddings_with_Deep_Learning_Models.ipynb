{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Exploring Word Embeddings with Deep Learning Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module04%20-%20Text%20Representation/Exploring_Word_Embeddings_with_Deep_Learning_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS9QXHRoRiqc",
        "colab_type": "text"
      },
      "source": [
        "# Text Representation with Feature Engineering\n",
        "\n",
        "### Project 7 - Exploring Word Embeddings with New Deep Learning Models\n",
        "\n",
        "We have discussed in the previous sub-unit that Feature Engineering is the secret sauce to creating superior and better performing machine learning models. \n",
        "\n",
        "Traditional (count-based) feature engineering strategies for textual data involve models belonging to a family of models popularly known as the Bag of Words model. This includes term frequencies, TF-IDF (term frequency-inverse document frequency), N-grams and so on. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence and context around nearby words in each text document. \n",
        "\n",
        "This forms as enough motivation for us to explore more sophisticated models which can capture this information and give us features which are vector representation of words, popularly known as embeddings.\n",
        "\n",
        "Here we will explore the following feature engineering techniques:\n",
        "\n",
        "- Word2Vec\n",
        "- GloVe\n",
        "- FastText\n",
        "\n",
        "Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process it learns distributed representations giving us dense word embeddings. We will be focusing on these predictive methods in this article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYUGSn3dRiqd",
        "colab_type": "text"
      },
      "source": [
        "# Prepare a Sample Corpus\n",
        "\n",
        "Let’s now take a sample corpus of documents on which we will run most of our analyses in this article. A corpus is typically a collection of text documents usually belonging to one or more subjects or domains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH4ZwBgtRiqe",
        "colab_type": "code",
        "outputId": "d62d533b-d8b6-4cbb-cd7e-2414b91fcd7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus, \n",
        "                          'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sky is blue and beautiful.</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this blue and beautiful sky!</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The dog is lazy but the brown fox is quick!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             Document Category\n",
              "0                                      The sky is blue and beautiful.  weather\n",
              "1                                   Love this blue and beautiful sky!  weather\n",
              "2                        The quick brown fox jumps over the lazy dog.  animals\n",
              "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
              "4                         I love green eggs, ham, sausages and bacon!     food\n",
              "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
              "6            The sky is very blue and the sky is very beautiful today  weather\n",
              "7                         The dog is lazy but the brown fox is quick!  animals"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLojJzK0Riqi",
        "colab_type": "text"
      },
      "source": [
        "Let's go ahead and pre-process our text data now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KR8w3qbRiqi",
        "colab_type": "text"
      },
      "source": [
        "# Simple Text Pre-processing\n",
        "\n",
        "Since the focus of this unit is on feature engineering, we will build a simple text pre-processor which focuses on removing special characters, extra whitespaces, digits, stopwords and lower casing the text corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YHHrL7VRiqj",
        "colab_type": "code",
        "outputId": "abf95d22-8394-469b-a29d-e1ae4de80c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(corpus)\n",
        "norm_corpus"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sky blue beautiful', 'love blue beautiful sky',\n",
              "       'quick brown fox jumps lazy dog',\n",
              "       'kings breakfast sausages ham bacon eggs toast beans',\n",
              "       'love green eggs ham sausages bacon',\n",
              "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
              "       'dog lazy brown fox quick'], dtype='<U51')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JCyCJ6GRiql",
        "colab_type": "text"
      },
      "source": [
        "# The Word2Vec Model\n",
        "\n",
        "This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity. Essentially these are unsupervised models which can take in massive textual corpora, create a vocabulary of possible words and generate dense word embeddings for each word in the vector space representing that vocabulary. \n",
        "\n",
        "Usually you can specify the size of the word embedding vectors and the total number of vectors are essentially the size of the vocabulary. This makes the dimensionality of this dense vector space much lower than the high-dimensional sparse vector space built using traditional Bag of Words models.\n",
        "\n",
        "There are two different model architectures which can be leveraged by Word2Vec to create these word embedding representations. These include,\n",
        "\n",
        "- The Continuous Bag of Words (CBOW) Model\n",
        "- The Skip-gram Model\n",
        "\n",
        "## The Continuous Bag of Words (CBOW) Model\n",
        "\n",
        "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). \n",
        "\n",
        "Considering a simple sentence, ___“the quick brown fox jumps over the lazy dog”___, this can be pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
        "\n",
        "Thus the model tries to predict the __`target_word`__ based on the __`context_window`__ words.\n",
        "\n",
        "![](cbow_arch.png)\n",
        "\n",
        "\n",
        "## The Skip-gram Model\n",
        "\n",
        "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word). \n",
        "\n",
        "Considering our simple sentence from earlier, ___“the quick brown fox jumps over the lazy dog”___. If we used the CBOW model, we get pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
        "\n",
        "Now considering that the skip-gram model’s aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context __[quick, fox]__ given target word __‘brown’__ or __[the, brown]__ given target word __‘quick’__ and so on. \n",
        "\n",
        "Thus the model tries to predict the context_window words based on the target_word.\n",
        "\n",
        "![](skipgram_arch.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHD8pUx7Riqm",
        "colab_type": "text"
      },
      "source": [
        "# Robust Word2Vec Model with Gensim\n",
        "\n",
        "The __`gensim`__ framework, created by Radim Řehůřek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our sample toy corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n",
        "\n",
        "- __`size`:__ The word embedding dimensionality\n",
        "- __`window`:__ The context window size\n",
        "- __`min_count`:__ The minimum word count\n",
        "- __`sample`:__ The downsample setting for frequent words\n",
        "- __`sg`:__ Training model, 1 for skip-gram otherwise CBOW\n",
        "\n",
        "We will build a simple Word2Vec model on the corpus and visualize the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tKqisDuRiqm",
        "colab_type": "code",
        "outputId": "f82b8161-1caa-4e5a-f811-cb68caa41ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "from gensim.models import word2vec\n",
        "\n",
        "tokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]\n",
        "\n",
        "# Set values for various parameters\n",
        "feature_size = 15    # Word vector dimensionality  \n",
        "window_context = 20  # Context window size                                                                                    \n",
        "min_word_count = 1   # Minimum word count                        \n",
        "sample = 1e-3        # Downsample setting for frequent words\n",
        "sg = 1               # skip-gram model\n",
        "\n",
        "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
        "                              window=window_context, min_count = min_word_count,\n",
        "                              sg=sg, sample=sample, iter=5000)\n",
        "w2v_model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7f24cd2bb748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NY4l-zaRiqo",
        "colab_type": "code",
        "outputId": "c754f40f-4f48-4062-da7c-65d6f9af1c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# visualize embeddings\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "words = w2v_model.wv.index2word\n",
        "wvs = w2v_model.wv[words]\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAFpCAYAAABqNGWjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7h2GmytRiqq",
        "colab_type": "code",
        "outputId": "084c9fdf-7966-4466-b77c-26de98568859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "w2v_model.wv['sky'], w2v_model.wv['sky'].shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.00751785, -0.6861029 ,  0.10433255,  0.2761935 , -0.73259896,\n",
              "        -0.5925274 , -0.46908614,  1.4228662 , -0.34438586, -1.3552268 ,\n",
              "         0.0140601 ,  0.65768677, -0.95163655, -0.27618387,  0.48435128],\n",
              "       dtype=float32), (15,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3INpkJK4Riqs",
        "colab_type": "code",
        "outputId": "6d0d46f2-22f4-40f0-b9b7-d64ddefb902b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "vec_df = pd.DataFrame(wvs, index=words)\n",
        "vec_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sky</th>\n",
              "      <td>-0.007518</td>\n",
              "      <td>-0.686103</td>\n",
              "      <td>0.104333</td>\n",
              "      <td>0.276193</td>\n",
              "      <td>-0.732599</td>\n",
              "      <td>-0.592527</td>\n",
              "      <td>-0.469086</td>\n",
              "      <td>1.422866</td>\n",
              "      <td>-0.344386</td>\n",
              "      <td>-1.355227</td>\n",
              "      <td>0.014060</td>\n",
              "      <td>0.657687</td>\n",
              "      <td>-0.951637</td>\n",
              "      <td>-0.276184</td>\n",
              "      <td>0.484351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>blue</th>\n",
              "      <td>-0.019332</td>\n",
              "      <td>-0.426953</td>\n",
              "      <td>0.070181</td>\n",
              "      <td>0.362138</td>\n",
              "      <td>-0.530271</td>\n",
              "      <td>-0.430891</td>\n",
              "      <td>-0.439222</td>\n",
              "      <td>0.815390</td>\n",
              "      <td>-0.348639</td>\n",
              "      <td>-0.954212</td>\n",
              "      <td>0.250487</td>\n",
              "      <td>0.544430</td>\n",
              "      <td>-0.685382</td>\n",
              "      <td>0.055481</td>\n",
              "      <td>-0.068991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beautiful</th>\n",
              "      <td>-0.309296</td>\n",
              "      <td>-0.585505</td>\n",
              "      <td>-0.116879</td>\n",
              "      <td>0.400632</td>\n",
              "      <td>-0.832173</td>\n",
              "      <td>-0.317873</td>\n",
              "      <td>-0.509451</td>\n",
              "      <td>1.267757</td>\n",
              "      <td>-0.352771</td>\n",
              "      <td>-1.075519</td>\n",
              "      <td>0.047094</td>\n",
              "      <td>0.436688</td>\n",
              "      <td>-0.882767</td>\n",
              "      <td>-0.432056</td>\n",
              "      <td>0.673438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quick</th>\n",
              "      <td>-0.962318</td>\n",
              "      <td>-0.743725</td>\n",
              "      <td>-0.090805</td>\n",
              "      <td>0.724243</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>-0.390891</td>\n",
              "      <td>-0.857201</td>\n",
              "      <td>0.447485</td>\n",
              "      <td>-0.308408</td>\n",
              "      <td>-0.805924</td>\n",
              "      <td>0.483776</td>\n",
              "      <td>-0.138345</td>\n",
              "      <td>-0.750049</td>\n",
              "      <td>0.151288</td>\n",
              "      <td>-1.424518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brown</th>\n",
              "      <td>-0.733915</td>\n",
              "      <td>-0.447475</td>\n",
              "      <td>0.099053</td>\n",
              "      <td>0.817001</td>\n",
              "      <td>0.245702</td>\n",
              "      <td>-0.381192</td>\n",
              "      <td>-1.062794</td>\n",
              "      <td>0.410750</td>\n",
              "      <td>-0.191799</td>\n",
              "      <td>-1.022655</td>\n",
              "      <td>0.646024</td>\n",
              "      <td>-0.205238</td>\n",
              "      <td>-0.587187</td>\n",
              "      <td>0.578950</td>\n",
              "      <td>-1.270139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fox</th>\n",
              "      <td>-0.917307</td>\n",
              "      <td>-0.731914</td>\n",
              "      <td>-0.179847</td>\n",
              "      <td>0.825812</td>\n",
              "      <td>-0.118211</td>\n",
              "      <td>-0.430267</td>\n",
              "      <td>-0.927723</td>\n",
              "      <td>0.358300</td>\n",
              "      <td>-0.098998</td>\n",
              "      <td>-0.879940</td>\n",
              "      <td>0.517486</td>\n",
              "      <td>-0.253078</td>\n",
              "      <td>-0.777871</td>\n",
              "      <td>0.213452</td>\n",
              "      <td>-1.410815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lazy</th>\n",
              "      <td>-1.086996</td>\n",
              "      <td>-0.513386</td>\n",
              "      <td>0.015318</td>\n",
              "      <td>0.584109</td>\n",
              "      <td>0.198123</td>\n",
              "      <td>-0.629645</td>\n",
              "      <td>-0.870751</td>\n",
              "      <td>0.375108</td>\n",
              "      <td>-0.187170</td>\n",
              "      <td>-1.041981</td>\n",
              "      <td>0.752502</td>\n",
              "      <td>-0.106188</td>\n",
              "      <td>-0.478407</td>\n",
              "      <td>0.686507</td>\n",
              "      <td>-1.185652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>-0.627311</td>\n",
              "      <td>-0.658254</td>\n",
              "      <td>-0.199163</td>\n",
              "      <td>0.890181</td>\n",
              "      <td>-0.151935</td>\n",
              "      <td>-0.519397</td>\n",
              "      <td>-1.005521</td>\n",
              "      <td>0.281845</td>\n",
              "      <td>-0.171195</td>\n",
              "      <td>-0.754155</td>\n",
              "      <td>0.706428</td>\n",
              "      <td>-0.177576</td>\n",
              "      <td>-0.632882</td>\n",
              "      <td>0.127297</td>\n",
              "      <td>-1.432362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.218343</td>\n",
              "      <td>-0.607394</td>\n",
              "      <td>0.125925</td>\n",
              "      <td>-0.125415</td>\n",
              "      <td>-0.200066</td>\n",
              "      <td>0.029339</td>\n",
              "      <td>-0.432395</td>\n",
              "      <td>0.860780</td>\n",
              "      <td>0.007130</td>\n",
              "      <td>-0.287942</td>\n",
              "      <td>0.463146</td>\n",
              "      <td>-0.169127</td>\n",
              "      <td>-0.333409</td>\n",
              "      <td>-0.257671</td>\n",
              "      <td>0.846600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sausages</th>\n",
              "      <td>0.051369</td>\n",
              "      <td>-0.697791</td>\n",
              "      <td>0.118476</td>\n",
              "      <td>-0.419413</td>\n",
              "      <td>0.339136</td>\n",
              "      <td>-0.302942</td>\n",
              "      <td>-0.849358</td>\n",
              "      <td>0.665032</td>\n",
              "      <td>-0.162119</td>\n",
              "      <td>0.191631</td>\n",
              "      <td>0.826756</td>\n",
              "      <td>-1.095855</td>\n",
              "      <td>-0.657710</td>\n",
              "      <td>-0.136995</td>\n",
              "      <td>0.972086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ham</th>\n",
              "      <td>-0.026397</td>\n",
              "      <td>-0.620554</td>\n",
              "      <td>0.231211</td>\n",
              "      <td>-0.709820</td>\n",
              "      <td>0.401656</td>\n",
              "      <td>-0.480261</td>\n",
              "      <td>-0.595770</td>\n",
              "      <td>0.857951</td>\n",
              "      <td>-0.132602</td>\n",
              "      <td>0.011542</td>\n",
              "      <td>0.706900</td>\n",
              "      <td>-1.055546</td>\n",
              "      <td>-0.544398</td>\n",
              "      <td>-0.531430</td>\n",
              "      <td>0.809204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacon</th>\n",
              "      <td>-0.102670</td>\n",
              "      <td>-0.811604</td>\n",
              "      <td>0.509146</td>\n",
              "      <td>-0.279226</td>\n",
              "      <td>0.593986</td>\n",
              "      <td>-0.302671</td>\n",
              "      <td>-0.733844</td>\n",
              "      <td>0.779734</td>\n",
              "      <td>-0.109385</td>\n",
              "      <td>0.098212</td>\n",
              "      <td>0.634572</td>\n",
              "      <td>-0.890824</td>\n",
              "      <td>-0.509274</td>\n",
              "      <td>-0.465287</td>\n",
              "      <td>1.009766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eggs</th>\n",
              "      <td>0.061611</td>\n",
              "      <td>-0.700913</td>\n",
              "      <td>0.224784</td>\n",
              "      <td>-0.559957</td>\n",
              "      <td>0.552240</td>\n",
              "      <td>-0.403576</td>\n",
              "      <td>-0.744116</td>\n",
              "      <td>0.658697</td>\n",
              "      <td>-0.006821</td>\n",
              "      <td>-0.068817</td>\n",
              "      <td>0.790205</td>\n",
              "      <td>-1.100820</td>\n",
              "      <td>-0.516152</td>\n",
              "      <td>-0.492826</td>\n",
              "      <td>0.829579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jumps</th>\n",
              "      <td>-0.754492</td>\n",
              "      <td>-0.584898</td>\n",
              "      <td>0.047061</td>\n",
              "      <td>0.687325</td>\n",
              "      <td>0.234426</td>\n",
              "      <td>-0.450875</td>\n",
              "      <td>-0.924435</td>\n",
              "      <td>0.200234</td>\n",
              "      <td>-0.175234</td>\n",
              "      <td>-0.733956</td>\n",
              "      <td>0.668797</td>\n",
              "      <td>-0.282459</td>\n",
              "      <td>-0.588700</td>\n",
              "      <td>0.458969</td>\n",
              "      <td>-1.357689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kings</th>\n",
              "      <td>0.055547</td>\n",
              "      <td>-1.083354</td>\n",
              "      <td>0.340160</td>\n",
              "      <td>-0.439667</td>\n",
              "      <td>0.855861</td>\n",
              "      <td>-0.267382</td>\n",
              "      <td>-1.056360</td>\n",
              "      <td>0.275948</td>\n",
              "      <td>-0.408272</td>\n",
              "      <td>0.026053</td>\n",
              "      <td>0.489345</td>\n",
              "      <td>-1.452088</td>\n",
              "      <td>-0.577965</td>\n",
              "      <td>-0.588913</td>\n",
              "      <td>0.588483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breakfast</th>\n",
              "      <td>-0.245105</td>\n",
              "      <td>-0.873291</td>\n",
              "      <td>0.517531</td>\n",
              "      <td>-0.292140</td>\n",
              "      <td>0.635383</td>\n",
              "      <td>-0.217093</td>\n",
              "      <td>-0.943770</td>\n",
              "      <td>0.389702</td>\n",
              "      <td>0.022208</td>\n",
              "      <td>0.116162</td>\n",
              "      <td>0.828406</td>\n",
              "      <td>-1.673895</td>\n",
              "      <td>-0.579864</td>\n",
              "      <td>-0.348554</td>\n",
              "      <td>0.764321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>toast</th>\n",
              "      <td>0.015311</td>\n",
              "      <td>-1.118451</td>\n",
              "      <td>0.469769</td>\n",
              "      <td>-0.522302</td>\n",
              "      <td>0.842120</td>\n",
              "      <td>-0.252062</td>\n",
              "      <td>-0.996350</td>\n",
              "      <td>0.355751</td>\n",
              "      <td>-0.358964</td>\n",
              "      <td>0.130463</td>\n",
              "      <td>0.758919</td>\n",
              "      <td>-1.560008</td>\n",
              "      <td>-0.549356</td>\n",
              "      <td>-0.477688</td>\n",
              "      <td>0.517930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beans</th>\n",
              "      <td>-0.078916</td>\n",
              "      <td>-1.095213</td>\n",
              "      <td>0.451517</td>\n",
              "      <td>-0.559120</td>\n",
              "      <td>0.738699</td>\n",
              "      <td>-0.147451</td>\n",
              "      <td>-1.104909</td>\n",
              "      <td>0.480080</td>\n",
              "      <td>-0.326229</td>\n",
              "      <td>0.034962</td>\n",
              "      <td>0.532533</td>\n",
              "      <td>-1.574777</td>\n",
              "      <td>-0.583417</td>\n",
              "      <td>-0.378745</td>\n",
              "      <td>0.788282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>green</th>\n",
              "      <td>0.331539</td>\n",
              "      <td>-0.627861</td>\n",
              "      <td>0.210382</td>\n",
              "      <td>-0.410307</td>\n",
              "      <td>0.484408</td>\n",
              "      <td>-0.353899</td>\n",
              "      <td>-0.503824</td>\n",
              "      <td>0.913308</td>\n",
              "      <td>-0.397367</td>\n",
              "      <td>0.017875</td>\n",
              "      <td>0.751835</td>\n",
              "      <td>-0.379551</td>\n",
              "      <td>-0.626228</td>\n",
              "      <td>-0.299961</td>\n",
              "      <td>0.961823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>today</th>\n",
              "      <td>-0.141265</td>\n",
              "      <td>-0.647149</td>\n",
              "      <td>-0.027899</td>\n",
              "      <td>0.325828</td>\n",
              "      <td>-0.937516</td>\n",
              "      <td>-0.145446</td>\n",
              "      <td>-0.570250</td>\n",
              "      <td>1.143511</td>\n",
              "      <td>-0.246465</td>\n",
              "      <td>-1.209200</td>\n",
              "      <td>0.044611</td>\n",
              "      <td>0.423886</td>\n",
              "      <td>-0.727595</td>\n",
              "      <td>-0.363828</td>\n",
              "      <td>0.567304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         1         2   ...        12        13        14\n",
              "sky       -0.007518 -0.686103  0.104333  ... -0.951637 -0.276184  0.484351\n",
              "blue      -0.019332 -0.426953  0.070181  ... -0.685382  0.055481 -0.068991\n",
              "beautiful -0.309296 -0.585505 -0.116879  ... -0.882767 -0.432056  0.673438\n",
              "quick     -0.962318 -0.743725 -0.090805  ... -0.750049  0.151288 -1.424518\n",
              "brown     -0.733915 -0.447475  0.099053  ... -0.587187  0.578950 -1.270139\n",
              "fox       -0.917307 -0.731914 -0.179847  ... -0.777871  0.213452 -1.410815\n",
              "lazy      -1.086996 -0.513386  0.015318  ... -0.478407  0.686507 -1.185652\n",
              "dog       -0.627311 -0.658254 -0.199163  ... -0.632882  0.127297 -1.432362\n",
              "love       0.218343 -0.607394  0.125925  ... -0.333409 -0.257671  0.846600\n",
              "sausages   0.051369 -0.697791  0.118476  ... -0.657710 -0.136995  0.972086\n",
              "ham       -0.026397 -0.620554  0.231211  ... -0.544398 -0.531430  0.809204\n",
              "bacon     -0.102670 -0.811604  0.509146  ... -0.509274 -0.465287  1.009766\n",
              "eggs       0.061611 -0.700913  0.224784  ... -0.516152 -0.492826  0.829579\n",
              "jumps     -0.754492 -0.584898  0.047061  ... -0.588700  0.458969 -1.357689\n",
              "kings      0.055547 -1.083354  0.340160  ... -0.577965 -0.588913  0.588483\n",
              "breakfast -0.245105 -0.873291  0.517531  ... -0.579864 -0.348554  0.764321\n",
              "toast      0.015311 -1.118451  0.469769  ... -0.549356 -0.477688  0.517930\n",
              "beans     -0.078916 -1.095213  0.451517  ... -0.583417 -0.378745  0.788282\n",
              "green      0.331539 -0.627861  0.210382  ... -0.626228 -0.299961  0.961823\n",
              "today     -0.141265 -0.647149 -0.027899  ... -0.727595 -0.363828  0.567304\n",
              "\n",
              "[20 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTCiDguVRiqu",
        "colab_type": "text"
      },
      "source": [
        "### Looking at term semantic similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S63FJhi2Riqv",
        "colab_type": "code",
        "outputId": "1fc21526-5314-4d7b-8594-4b1b2090c27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(vec_df.values)\n",
        "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
        "similarity_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sky</th>\n",
              "      <th>blue</th>\n",
              "      <th>beautiful</th>\n",
              "      <th>quick</th>\n",
              "      <th>brown</th>\n",
              "      <th>fox</th>\n",
              "      <th>lazy</th>\n",
              "      <th>dog</th>\n",
              "      <th>love</th>\n",
              "      <th>sausages</th>\n",
              "      <th>ham</th>\n",
              "      <th>bacon</th>\n",
              "      <th>eggs</th>\n",
              "      <th>jumps</th>\n",
              "      <th>kings</th>\n",
              "      <th>breakfast</th>\n",
              "      <th>toast</th>\n",
              "      <th>beans</th>\n",
              "      <th>green</th>\n",
              "      <th>today</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sky</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.946080</td>\n",
              "      <td>0.968517</td>\n",
              "      <td>0.439142</td>\n",
              "      <td>0.397034</td>\n",
              "      <td>0.428546</td>\n",
              "      <td>0.394244</td>\n",
              "      <td>0.405311</td>\n",
              "      <td>0.691241</td>\n",
              "      <td>0.298472</td>\n",
              "      <td>0.345592</td>\n",
              "      <td>0.347247</td>\n",
              "      <td>0.301156</td>\n",
              "      <td>0.311986</td>\n",
              "      <td>0.172020</td>\n",
              "      <td>0.137667</td>\n",
              "      <td>0.136412</td>\n",
              "      <td>0.196005</td>\n",
              "      <td>0.482503</td>\n",
              "      <td>0.966768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>blue</th>\n",
              "      <td>0.946080</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.894317</td>\n",
              "      <td>0.626863</td>\n",
              "      <td>0.607423</td>\n",
              "      <td>0.618088</td>\n",
              "      <td>0.597211</td>\n",
              "      <td>0.616418</td>\n",
              "      <td>0.544041</td>\n",
              "      <td>0.202102</td>\n",
              "      <td>0.209990</td>\n",
              "      <td>0.215017</td>\n",
              "      <td>0.182632</td>\n",
              "      <td>0.532897</td>\n",
              "      <td>0.090362</td>\n",
              "      <td>0.059599</td>\n",
              "      <td>0.070990</td>\n",
              "      <td>0.101378</td>\n",
              "      <td>0.364384</td>\n",
              "      <td>0.901874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beautiful</th>\n",
              "      <td>0.968517</td>\n",
              "      <td>0.894317</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.425173</td>\n",
              "      <td>0.357100</td>\n",
              "      <td>0.419854</td>\n",
              "      <td>0.356003</td>\n",
              "      <td>0.388646</td>\n",
              "      <td>0.724752</td>\n",
              "      <td>0.347932</td>\n",
              "      <td>0.373159</td>\n",
              "      <td>0.385190</td>\n",
              "      <td>0.327872</td>\n",
              "      <td>0.276214</td>\n",
              "      <td>0.201762</td>\n",
              "      <td>0.191498</td>\n",
              "      <td>0.161493</td>\n",
              "      <td>0.234392</td>\n",
              "      <td>0.481783</td>\n",
              "      <td>0.985901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quick</th>\n",
              "      <td>0.439142</td>\n",
              "      <td>0.626863</td>\n",
              "      <td>0.425173</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.954559</td>\n",
              "      <td>0.991699</td>\n",
              "      <td>0.944302</td>\n",
              "      <td>0.974879</td>\n",
              "      <td>0.086900</td>\n",
              "      <td>0.138760</td>\n",
              "      <td>0.122493</td>\n",
              "      <td>0.135618</td>\n",
              "      <td>0.141930</td>\n",
              "      <td>0.968819</td>\n",
              "      <td>0.231159</td>\n",
              "      <td>0.218717</td>\n",
              "      <td>0.238249</td>\n",
              "      <td>0.204424</td>\n",
              "      <td>0.071506</td>\n",
              "      <td>0.418990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brown</th>\n",
              "      <td>0.397034</td>\n",
              "      <td>0.607423</td>\n",
              "      <td>0.357100</td>\n",
              "      <td>0.954559</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.957728</td>\n",
              "      <td>0.974895</td>\n",
              "      <td>0.951023</td>\n",
              "      <td>0.088283</td>\n",
              "      <td>0.163117</td>\n",
              "      <td>0.113488</td>\n",
              "      <td>0.147386</td>\n",
              "      <td>0.159028</td>\n",
              "      <td>0.984403</td>\n",
              "      <td>0.229878</td>\n",
              "      <td>0.244789</td>\n",
              "      <td>0.244549</td>\n",
              "      <td>0.216227</td>\n",
              "      <td>0.087963</td>\n",
              "      <td>0.365178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fox</th>\n",
              "      <td>0.428546</td>\n",
              "      <td>0.618088</td>\n",
              "      <td>0.419854</td>\n",
              "      <td>0.991699</td>\n",
              "      <td>0.957728</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.943347</td>\n",
              "      <td>0.985326</td>\n",
              "      <td>0.089077</td>\n",
              "      <td>0.142190</td>\n",
              "      <td>0.110497</td>\n",
              "      <td>0.120517</td>\n",
              "      <td>0.140617</td>\n",
              "      <td>0.970848</td>\n",
              "      <td>0.219376</td>\n",
              "      <td>0.225900</td>\n",
              "      <td>0.226101</td>\n",
              "      <td>0.195513</td>\n",
              "      <td>0.043527</td>\n",
              "      <td>0.420978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lazy</th>\n",
              "      <td>0.394244</td>\n",
              "      <td>0.597211</td>\n",
              "      <td>0.356003</td>\n",
              "      <td>0.944302</td>\n",
              "      <td>0.974895</td>\n",
              "      <td>0.943347</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>0.067496</td>\n",
              "      <td>0.154362</td>\n",
              "      <td>0.124189</td>\n",
              "      <td>0.136852</td>\n",
              "      <td>0.153940</td>\n",
              "      <td>0.969828</td>\n",
              "      <td>0.193790</td>\n",
              "      <td>0.225021</td>\n",
              "      <td>0.217238</td>\n",
              "      <td>0.189698</td>\n",
              "      <td>0.079480</td>\n",
              "      <td>0.352452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>0.405311</td>\n",
              "      <td>0.616418</td>\n",
              "      <td>0.388646</td>\n",
              "      <td>0.974879</td>\n",
              "      <td>0.951023</td>\n",
              "      <td>0.985326</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.085408</td>\n",
              "      <td>0.141333</td>\n",
              "      <td>0.100393</td>\n",
              "      <td>0.107976</td>\n",
              "      <td>0.137802</td>\n",
              "      <td>0.967890</td>\n",
              "      <td>0.215202</td>\n",
              "      <td>0.210882</td>\n",
              "      <td>0.224600</td>\n",
              "      <td>0.177520</td>\n",
              "      <td>0.056350</td>\n",
              "      <td>0.393658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.691241</td>\n",
              "      <td>0.544041</td>\n",
              "      <td>0.724752</td>\n",
              "      <td>0.086900</td>\n",
              "      <td>0.088283</td>\n",
              "      <td>0.089077</td>\n",
              "      <td>0.067496</td>\n",
              "      <td>0.085408</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.783488</td>\n",
              "      <td>0.775271</td>\n",
              "      <td>0.799072</td>\n",
              "      <td>0.765815</td>\n",
              "      <td>0.029677</td>\n",
              "      <td>0.601239</td>\n",
              "      <td>0.631058</td>\n",
              "      <td>0.600880</td>\n",
              "      <td>0.659193</td>\n",
              "      <td>0.857228</td>\n",
              "      <td>0.729267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sausages</th>\n",
              "      <td>0.298472</td>\n",
              "      <td>0.202102</td>\n",
              "      <td>0.347932</td>\n",
              "      <td>0.138760</td>\n",
              "      <td>0.163117</td>\n",
              "      <td>0.142190</td>\n",
              "      <td>0.154362</td>\n",
              "      <td>0.141333</td>\n",
              "      <td>0.783488</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.954169</td>\n",
              "      <td>0.951253</td>\n",
              "      <td>0.966685</td>\n",
              "      <td>0.171031</td>\n",
              "      <td>0.899297</td>\n",
              "      <td>0.935090</td>\n",
              "      <td>0.914469</td>\n",
              "      <td>0.934412</td>\n",
              "      <td>0.909865</td>\n",
              "      <td>0.311227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ham</th>\n",
              "      <td>0.345592</td>\n",
              "      <td>0.209990</td>\n",
              "      <td>0.373159</td>\n",
              "      <td>0.122493</td>\n",
              "      <td>0.113488</td>\n",
              "      <td>0.110497</td>\n",
              "      <td>0.124189</td>\n",
              "      <td>0.100393</td>\n",
              "      <td>0.775271</td>\n",
              "      <td>0.954169</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.954262</td>\n",
              "      <td>0.984897</td>\n",
              "      <td>0.124348</td>\n",
              "      <td>0.891773</td>\n",
              "      <td>0.906724</td>\n",
              "      <td>0.905010</td>\n",
              "      <td>0.917000</td>\n",
              "      <td>0.914582</td>\n",
              "      <td>0.331838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacon</th>\n",
              "      <td>0.347247</td>\n",
              "      <td>0.215017</td>\n",
              "      <td>0.385190</td>\n",
              "      <td>0.135618</td>\n",
              "      <td>0.147386</td>\n",
              "      <td>0.120517</td>\n",
              "      <td>0.136852</td>\n",
              "      <td>0.107976</td>\n",
              "      <td>0.799072</td>\n",
              "      <td>0.951253</td>\n",
              "      <td>0.954262</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.965723</td>\n",
              "      <td>0.147104</td>\n",
              "      <td>0.910823</td>\n",
              "      <td>0.932832</td>\n",
              "      <td>0.917406</td>\n",
              "      <td>0.937344</td>\n",
              "      <td>0.919375</td>\n",
              "      <td>0.343941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eggs</th>\n",
              "      <td>0.301156</td>\n",
              "      <td>0.182632</td>\n",
              "      <td>0.327872</td>\n",
              "      <td>0.141930</td>\n",
              "      <td>0.159028</td>\n",
              "      <td>0.140617</td>\n",
              "      <td>0.153940</td>\n",
              "      <td>0.137802</td>\n",
              "      <td>0.765815</td>\n",
              "      <td>0.966685</td>\n",
              "      <td>0.984897</td>\n",
              "      <td>0.965723</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.169392</td>\n",
              "      <td>0.929003</td>\n",
              "      <td>0.944421</td>\n",
              "      <td>0.938856</td>\n",
              "      <td>0.945815</td>\n",
              "      <td>0.903971</td>\n",
              "      <td>0.297637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jumps</th>\n",
              "      <td>0.311986</td>\n",
              "      <td>0.532897</td>\n",
              "      <td>0.276214</td>\n",
              "      <td>0.968819</td>\n",
              "      <td>0.984403</td>\n",
              "      <td>0.970848</td>\n",
              "      <td>0.969828</td>\n",
              "      <td>0.967890</td>\n",
              "      <td>0.029677</td>\n",
              "      <td>0.171031</td>\n",
              "      <td>0.124348</td>\n",
              "      <td>0.147104</td>\n",
              "      <td>0.169392</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.264055</td>\n",
              "      <td>0.272484</td>\n",
              "      <td>0.284463</td>\n",
              "      <td>0.238260</td>\n",
              "      <td>0.069237</td>\n",
              "      <td>0.278244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kings</th>\n",
              "      <td>0.172020</td>\n",
              "      <td>0.090362</td>\n",
              "      <td>0.201762</td>\n",
              "      <td>0.231159</td>\n",
              "      <td>0.229878</td>\n",
              "      <td>0.219376</td>\n",
              "      <td>0.193790</td>\n",
              "      <td>0.215202</td>\n",
              "      <td>0.601239</td>\n",
              "      <td>0.899297</td>\n",
              "      <td>0.891773</td>\n",
              "      <td>0.910823</td>\n",
              "      <td>0.929003</td>\n",
              "      <td>0.264055</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.950161</td>\n",
              "      <td>0.990295</td>\n",
              "      <td>0.985835</td>\n",
              "      <td>0.793372</td>\n",
              "      <td>0.178961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breakfast</th>\n",
              "      <td>0.137667</td>\n",
              "      <td>0.059599</td>\n",
              "      <td>0.191498</td>\n",
              "      <td>0.218717</td>\n",
              "      <td>0.244789</td>\n",
              "      <td>0.225900</td>\n",
              "      <td>0.225021</td>\n",
              "      <td>0.210882</td>\n",
              "      <td>0.631058</td>\n",
              "      <td>0.935090</td>\n",
              "      <td>0.906724</td>\n",
              "      <td>0.932832</td>\n",
              "      <td>0.944421</td>\n",
              "      <td>0.272484</td>\n",
              "      <td>0.950161</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.968385</td>\n",
              "      <td>0.971819</td>\n",
              "      <td>0.771537</td>\n",
              "      <td>0.166468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>toast</th>\n",
              "      <td>0.136412</td>\n",
              "      <td>0.070990</td>\n",
              "      <td>0.161493</td>\n",
              "      <td>0.238249</td>\n",
              "      <td>0.244549</td>\n",
              "      <td>0.226101</td>\n",
              "      <td>0.217238</td>\n",
              "      <td>0.224600</td>\n",
              "      <td>0.600880</td>\n",
              "      <td>0.914469</td>\n",
              "      <td>0.905010</td>\n",
              "      <td>0.917406</td>\n",
              "      <td>0.938856</td>\n",
              "      <td>0.284463</td>\n",
              "      <td>0.990295</td>\n",
              "      <td>0.968385</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.986597</td>\n",
              "      <td>0.798917</td>\n",
              "      <td>0.140738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beans</th>\n",
              "      <td>0.196005</td>\n",
              "      <td>0.101378</td>\n",
              "      <td>0.234392</td>\n",
              "      <td>0.204424</td>\n",
              "      <td>0.216227</td>\n",
              "      <td>0.195513</td>\n",
              "      <td>0.189698</td>\n",
              "      <td>0.177520</td>\n",
              "      <td>0.659193</td>\n",
              "      <td>0.934412</td>\n",
              "      <td>0.917000</td>\n",
              "      <td>0.937344</td>\n",
              "      <td>0.945815</td>\n",
              "      <td>0.238260</td>\n",
              "      <td>0.985835</td>\n",
              "      <td>0.971819</td>\n",
              "      <td>0.986597</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.812636</td>\n",
              "      <td>0.215397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>green</th>\n",
              "      <td>0.482503</td>\n",
              "      <td>0.364384</td>\n",
              "      <td>0.481783</td>\n",
              "      <td>0.071506</td>\n",
              "      <td>0.087963</td>\n",
              "      <td>0.043527</td>\n",
              "      <td>0.079480</td>\n",
              "      <td>0.056350</td>\n",
              "      <td>0.857228</td>\n",
              "      <td>0.909865</td>\n",
              "      <td>0.914582</td>\n",
              "      <td>0.919375</td>\n",
              "      <td>0.903971</td>\n",
              "      <td>0.069237</td>\n",
              "      <td>0.793372</td>\n",
              "      <td>0.771537</td>\n",
              "      <td>0.798917</td>\n",
              "      <td>0.812636</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.436454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>today</th>\n",
              "      <td>0.966768</td>\n",
              "      <td>0.901874</td>\n",
              "      <td>0.985901</td>\n",
              "      <td>0.418990</td>\n",
              "      <td>0.365178</td>\n",
              "      <td>0.420978</td>\n",
              "      <td>0.352452</td>\n",
              "      <td>0.393658</td>\n",
              "      <td>0.729267</td>\n",
              "      <td>0.311227</td>\n",
              "      <td>0.331838</td>\n",
              "      <td>0.343941</td>\n",
              "      <td>0.297637</td>\n",
              "      <td>0.278244</td>\n",
              "      <td>0.178961</td>\n",
              "      <td>0.166468</td>\n",
              "      <td>0.140738</td>\n",
              "      <td>0.215397</td>\n",
              "      <td>0.436454</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                sky      blue  beautiful  ...     beans     green     today\n",
              "sky        1.000000  0.946080   0.968517  ...  0.196005  0.482503  0.966768\n",
              "blue       0.946080  1.000000   0.894317  ...  0.101378  0.364384  0.901874\n",
              "beautiful  0.968517  0.894317   1.000000  ...  0.234392  0.481783  0.985901\n",
              "quick      0.439142  0.626863   0.425173  ...  0.204424  0.071506  0.418990\n",
              "brown      0.397034  0.607423   0.357100  ...  0.216227  0.087963  0.365178\n",
              "fox        0.428546  0.618088   0.419854  ...  0.195513  0.043527  0.420978\n",
              "lazy       0.394244  0.597211   0.356003  ...  0.189698  0.079480  0.352452\n",
              "dog        0.405311  0.616418   0.388646  ...  0.177520  0.056350  0.393658\n",
              "love       0.691241  0.544041   0.724752  ...  0.659193  0.857228  0.729267\n",
              "sausages   0.298472  0.202102   0.347932  ...  0.934412  0.909865  0.311227\n",
              "ham        0.345592  0.209990   0.373159  ...  0.917000  0.914582  0.331838\n",
              "bacon      0.347247  0.215017   0.385190  ...  0.937344  0.919375  0.343941\n",
              "eggs       0.301156  0.182632   0.327872  ...  0.945815  0.903971  0.297637\n",
              "jumps      0.311986  0.532897   0.276214  ...  0.238260  0.069237  0.278244\n",
              "kings      0.172020  0.090362   0.201762  ...  0.985835  0.793372  0.178961\n",
              "breakfast  0.137667  0.059599   0.191498  ...  0.971819  0.771537  0.166468\n",
              "toast      0.136412  0.070990   0.161493  ...  0.986597  0.798917  0.140738\n",
              "beans      0.196005  0.101378   0.234392  ...  1.000000  0.812636  0.215397\n",
              "green      0.482503  0.364384   0.481783  ...  0.812636  1.000000  0.436454\n",
              "today      0.966768  0.901874   0.985901  ...  0.215397  0.436454  1.000000\n",
              "\n",
              "[20 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngIHnn__Riqx",
        "colab_type": "code",
        "outputId": "4ae91cf2-2159-4883-bc69-1b6525e527d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "feature_names = np.array(words)\n",
        "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
        "                    axis=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sky           [beautiful, today, blue]\n",
              "blue           [sky, today, beautiful]\n",
              "beautiful           [today, sky, blue]\n",
              "quick                [fox, dog, jumps]\n",
              "brown               [jumps, lazy, fox]\n",
              "fox                [quick, dog, jumps]\n",
              "lazy             [brown, jumps, quick]\n",
              "dog                [fox, quick, jumps]\n",
              "love          [green, bacon, sausages]\n",
              "sausages            [eggs, ham, bacon]\n",
              "ham            [eggs, bacon, sausages]\n",
              "bacon            [eggs, ham, sausages]\n",
              "eggs            [ham, sausages, bacon]\n",
              "jumps               [brown, fox, lazy]\n",
              "kings        [toast, beans, breakfast]\n",
              "breakfast        [beans, toast, kings]\n",
              "toast        [kings, beans, breakfast]\n",
              "beans        [toast, kings, breakfast]\n",
              "green           [bacon, ham, sausages]\n",
              "today           [beautiful, sky, blue]\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTUAkYxQRiqz",
        "colab_type": "text"
      },
      "source": [
        "# The GloVe Model\n",
        "\n",
        "The GloVe model stands for Global Vectors which is an unsupervised learning model which can be used to obtain dense word vectors similar to Word2Vec. However the technique is different and training is performed on an aggregated global word-word co-occurrence matrix, giving us a vector space with meaningful sub-structures. This method was invented in Stanford by Pennington et al. and I recommend you to read the original paper on GloVe, _[‘GloVe: Global Vectors for Word Representation’ by Pennington et al.](https://nlp.stanford.edu/pubs/glove.pdf)_ which is an excellent read to get some perspective on how this model works.\n",
        "\n",
        "The basic methodology of the GloVe model is to first create a huge word-context co-occurence matrix consisting of (word, context) pairs such that each element in this matrix represents how often a word occurs with the context (which can be a sequence of words). The idea then is to apply matrix factorization to approximate this matrix as depicted in the following figure.\n",
        "\n",
        "![](glove_arch.png)\n",
        "\n",
        "Considering the __Word-Context (WC)__ matrix, __Word-Feature (WF)__ matrix and __Feature-Context (FC)__ matrix, we try to factorize __WC = WF x FC__\n",
        "\n",
        "Such that we we aim to reconstruct __WC__ from __WF__ and __FC__ by multiplying them. For this, we typically initialize __WF__ and __FC__ with some random weights and attempt to multiply them to get __WC'__ (an approximation of __WC__) and measure how close it is to __WC__. We do this multiple times using Stochastic Gradient Descent (SGD) to minimize the error. Finally, the __Word-Feature matrix (WF)__ gives us the word embeddings for each word where __F__ can be preset to a specific number of dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UFyeFqfRiq0",
        "colab_type": "text"
      },
      "source": [
        "# Robust Glove Model with SpaCy\n",
        "\n",
        "Let’s try and leverage GloVe based embeddings for our document clustering task. The very popular spacy framework comes with capabilities to leverage GloVe embeddings based on different language models. You can also get pre-trained word vectors and load them up as needed using gensim or spacy.\n",
        "\n",
        "If you have spacy installed, we will be using the __[`en_vectors_web_lg`](https://spacy.io/models/en#en_vectors_web_lg)__ model which consists of 300-dimensional word vectors trained on [Common Crawl](http://commoncrawl.org) with GloVe.\n",
        "\n",
        "__Install Instructions:__\n",
        "\n",
        "```\n",
        "# Use the following command to install spaCy\n",
        "> pip install -U spacy\n",
        "OR\n",
        "> conda install -c conda-forge spacy\n",
        "\n",
        "C:\\WINDOWS\\system32>python -m spacy download en_vectors_web_lg\n",
        "Collecting en_vectors_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz#egg=en_vectors_web_lg==2.0.0\n",
        "  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz (661.8MB)\n",
        "    100% |████████████████████████████████| 661.8MB 392kB/s\n",
        "Installing collected packages: en-vectors-web-lg\n",
        "  Running setup.py install for en-vectors-web-lg ... done\n",
        "Successfully installed en-vectors-web-lg-2.0.0\n",
        "You are using pip version 10.0.1, however version 18.0 is available.\n",
        "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
        "\n",
        "    Linking successful\n",
        "    C:\\Anaconda3\\lib\\site-packages\\en_vectors_web_lg -->\n",
        "    C:\\Anaconda3\\lib\\site-packages\\spacy\\data\\en_vectors_web_lg\n",
        "\n",
        "    You can now load the model via spacy.load('en_vectors_web_lg')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geEhPWzgTA7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "f8d35ab0-f16c-4b95-9080-857cb815a493"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz4gggzKTkYx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "48ae1515-c384-449e-f280-e39019de3319"
      },
      "source": [
        "!python -m spacy download en_vectors_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_vectors_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.1.0/en_vectors_web_lg-2.1.0.tar.gz#egg=en_vectors_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.1.0/en_vectors_web_lg-2.1.0.tar.gz (661.8MB)\n",
            "\u001b[K     |████████████████████████████████| 661.8MB 1.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-vectors-web-lg\n",
            "  Building wheel for en-vectors-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-vectors-web-lg: filename=en_vectors_web_lg-2.1.0-cp36-none-any.whl size=663461747 sha256=fab022ae7ff2358ce5ea03a19303fb5b51b03dd98f69d8883ee8293fd01f1d71\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f6bunmnw/wheels/ce/3e/83/59647d0b4584003cce18fb68ecda2866e7c7b2722c3ecaddaf\n",
            "Successfully built en-vectors-web-lg\n",
            "Installing collected packages: en-vectors-web-lg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfnNf5fRiq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_vectors_web_lg')\n",
        "total_vectors = len(nlp.vocab.vectors)\n",
        "\n",
        "print('Total word vectors:', total_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gf4dcBuRiq2",
        "colab_type": "text"
      },
      "source": [
        "This validates that everything is working and in order. Let’s get the GloVe embeddings for each of our words now in our toy corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd68_DV0Riq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words = list(set([word for sublist in tokenized_corpus for word in sublist]))\n",
        "\n",
        "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])\n",
        "vec_df = pd.DataFrame(word_glove_vectors, index=unique_words)\n",
        "vec_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSKGVbkeRiq5",
        "colab_type": "text"
      },
      "source": [
        "We can now use t-SNE to visualize these embeddings similar to what we did using our Word2Vec embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6aSesHGRiq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(word_glove_vectors)\n",
        "labels = unique_words\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='red', edgecolors='r')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce6BV5bFRiq7",
        "colab_type": "text"
      },
      "source": [
        "### Looking at term semantic similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyO3JzS2Riq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(vec_df.values)\n",
        "similarity_df = pd.DataFrame(similarity_matrix, index=unique_words, columns=unique_words)\n",
        "similarity_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC-aQ1IERiq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = np.array(unique_words)\n",
        "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
        "                    axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU1BAEUWRiq_",
        "colab_type": "text"
      },
      "source": [
        "# The FastText Model\n",
        "\n",
        "The FastText model was first introduced by Facebook in 2016 as an extension and supposedly improvement of the vanilla Word2Vec model. Based on the original paper titled _[‘Enriching Word Vectors with Subword Information’](https://arxiv.org/pdf/1607.04606.pdf)_ by Mikolov et al. which is an excellent read to gain an in-depth understanding of how this model works. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on [GitHub](https://github.com/facebookresearch/fastText) and claims to have the following.\n",
        "\n",
        "- Recent state-of-the-art English word vectors.\n",
        "- Word vectors for 157 languages trained on Wikipedia and Crawl.\n",
        "- Models for language identification and various supervised tasks.\n",
        "\n",
        "Though I haven't implemented this model from scratch, based on the research paper, following is what I learnt about how the model works. In general, predictive models like the Word2Vec model typically considers each word as a distinct entity (e.g. where) and generates a dense embedding for the word. However this poses to be a serious limitation with languages having massive vocabularies and many rare words which may not occur a lot in different corpora. \n",
        "\n",
        "The Word2Vec model typically ignores the morphological structure of each word and considers a word as a single entity. The FastText model ___considers each word as a Bag of Character n-grams___. This is also called as a ___subword model___ in the paper.\n",
        "\n",
        "We add special boundary symbols __<__ and __>__ at the beginning and end of words. This enables us to distinguish prefixes and suffixes from other character sequences. We also include the word __w__ itself in the set of its n-grams, to learn a representation for each word (in addition to its character n-grams). \n",
        "\n",
        "Taking the word where and __n=3 (tri-grams)__ as an example, it will be represented by the __character n-grams__: __<wh, whe, her, ere, re>__ and the special sequence __< where >__ representing the whole word. Note that the sequence , corresponding to the word __< her >__ is different from the tri-gram __her__ from the word __where__.\n",
        "\n",
        "In practice, the paper recommends in extracting all the n-grams for __n ≥ 3__ and __n ≤ 6__. This is a very simple approach, and different sets of n-grams could be considered, for example taking all prefixes and suffixes. We typically associate a vector representation (embedding) to each n-gram for a word. \n",
        "\n",
        "Thus, we can represent a word by the sum of the vector representations of its n-grams or the average of the embedding of these n-grams. Thus, due to this effect of leveraging n-grams from individual words based on their characters, there is a higher chance for rare words to get a good representation since their character based n-grams should occur across other words of the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTpq6CyxRiq_",
        "colab_type": "text"
      },
      "source": [
        "# Robust FastText Model with Gensim\n",
        "\n",
        "The __`gensim`__ package has nice wrappers providing us interfaces to leverage the FastText model available under the `gensim.models.fasttext` module. Let’s apply this once again on our toy corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzrr2AiJRirA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "# Set values for various parameters\n",
        "feature_size = 15    # Word vector dimensionality  \n",
        "window_context = 20  # Context window size                                                                                    \n",
        "min_word_count = 1   # Minimum word count                        \n",
        "sample = 1e-3        # Downsample setting for frequent words\n",
        "sg = 1               # skip-gram model\n",
        "\n",
        "ft_model = FastText(tokenized_corpus, size=feature_size, \n",
        "                     window=window_context, min_count = min_word_count,\n",
        "                     sg=sg, sample=sample, iter=5000)\n",
        "ft_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR6sYaASRirB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize embeddings\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "words = ft_model.wv.index2word\n",
        "wvs = ft_model.wv[words]\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='green', edgecolors='k')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LplYuHBRirE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ft_model.wv['sky'], ft_model.wv['sky'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40z0JRH5RirG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(ft_model.wv.similarity(w1='ham', w2='sky'))\n",
        "print(ft_model.wv.similarity(w1='ham', w2='sausages'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iwDNr-lRirI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st1 = \"dog fox ham\"\n",
        "print('Odd one out for [',st1, ']:',  \n",
        "      ft_model.wv.doesnt_match(st1.split()))\n",
        "\n",
        "st2 = \"bacon ham sky sausages\"\n",
        "print('Odd one out for [',st2, ']:', \n",
        "      ft_model.wv.doesnt_match(st2.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxZeE7w8RirK",
        "colab_type": "text"
      },
      "source": [
        "### Getting document level embeddings\n",
        "\n",
        "Now suppose we wanted to cluster the eight documents from our toy corpus, we would need to get the document level embeddings from each of the words present in each document. One strategy would be to average out the word embeddings for each word in a document. This is an extremely useful strategy and you can adopt the same for your own problems. Let’s apply this now on our corpus to get features for each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ETS-PxRirK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    \n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "    \n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def averaged_word_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPFoB8y5RirM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get document level embeddings\n",
        "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n",
        "                                             num_features=feature_size)\n",
        "pd.DataFrame(ft_doc_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgrKl_vqRirN",
        "colab_type": "text"
      },
      "source": [
        "### Trying out document clustering\n",
        "\n",
        "Now that we have our features for each document, let’s cluster these documents using the Affinity Propagation algorithm, which is a clustering algorithm based on the concept of “message passing” between data points and does not need the number of clusters as an explicit input which is often required by partition-based clustering algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9C0aRLwRirO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "ap = AffinityPropagation()\n",
        "ap.fit(ft_doc_features)\n",
        "\n",
        "cluster_labels = ap.labels_\n",
        "cluster_labels = pd.DataFrame(cluster_labels, \n",
        "                              columns=['ClusterLabel'])\n",
        "\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB6pQQTPRirQ",
        "colab_type": "text"
      },
      "source": [
        "We can see that our algorithm has clustered each document into the right group based on our Word2Vec features. Pretty neat! We can also visualize how each document in positioned in each cluster by using [_Principal Component Analysis (PCA)_](https://en.wikipedia.org/wiki/Principal_component_analysis) to reduce the feature dimensions to 2-D and then visualizing the same (by color coding each cluster)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VA6oJnIRirR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "pcs = pca.fit_transform(ft_doc_features)\n",
        "labels = ap.labels_\n",
        "categories = list(corpus_df['Category'])\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    label = labels[i]\n",
        "    color = 'orange' if label == 0 else 'blue' if label == 1 else 'green'\n",
        "    annotation_label = categories[i]\n",
        "    x, y = pcs[i]\n",
        "    plt.scatter(x, y, c=color, edgecolors='k')\n",
        "    plt.annotate(annotation_label, xy=(x+1e-2, y+1e-2), xytext=(0, 0), \n",
        "                 textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvFiGdkMRirS",
        "colab_type": "text"
      },
      "source": [
        "Everything looks to be in order as documents in each cluster are closer to each other and far apart from other clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr_ejT8_Tsq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}